{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d46f884",
   "metadata": {},
   "source": [
    "# Building a Decision Tree Model For Anova Insurance (Google Colab Notebook)\n",
    "\n",
    "This notebook follows a **step-by-step, end-to-end workflow** to build an **interpretable Decision Tree classifier** that predicts applicant health status:\n",
    "- **0 = Healthy**\n",
    "- **1 = Unhealthy**\n",
    "\n",
    "It also exports final outputs for download:\n",
    "1. **Predictions CSV**\n",
    "2. **Excel workbook (2 sheets/tabs)**:\n",
    "   - `1_Decision Tree Model For Anova Insurance`\n",
    "   - `2_Decision Tree Success Criteria`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f29d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Step 0: Setup (Install/Import)\n",
    "# ==============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Colab-only: file upload and download helpers\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(\"✅ Imports complete. IN_COLAB =\", IN_COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# Step 5.1: Data Ingestion (Upload CSV/XLSX in Colab)\n",
    "# ======================================================\n",
    "# You will be prompted to upload your dataset file.\n",
    "# Supported: .csv, .xlsx\n",
    "\n",
    "if IN_COLAB:\n",
    "    uploaded = files.upload()  # choose your file\n",
    "    uploaded_filename = list(uploaded.keys())[0]\n",
    "    print(\"Uploaded file:\", uploaded_filename)\n",
    "else:\n",
    "    # If not in Colab, set your local path here:\n",
    "    uploaded_filename = \"YOUR_DATA_FILE.csv\"\n",
    "    print(\"Running outside Colab. Using:\", uploaded_filename)\n",
    "\n",
    "def load_dataset(path: str) -> pd.DataFrame:\n",
    "    if path.lower().endswith(\".csv\"):\n",
    "        return pd.read_csv(path)\n",
    "    if path.lower().endswith(\".xlsx\") or path.lower().endswith(\".xls\"):\n",
    "        # Read the first sheet by default\n",
    "        return pd.read_excel(path)\n",
    "    raise ValueError(\"Unsupported file type. Please upload a .csv or .xlsx file.\")\n",
    "\n",
    "df = load_dataset(uploaded_filename)\n",
    "\n",
    "print(\"✅ Loaded dataset.\")\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66c2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Step 5.1 (cont.): Basic Data Validation\n",
    "# =========================================\n",
    "required_target = \"Target\"\n",
    "if required_target not in df.columns:\n",
    "    raise ValueError(f\"Missing required target column: '{required_target}'. Found columns: {list(df.columns)}\")\n",
    "\n",
    "# Basic checks\n",
    "print(\"Target value counts:\\n\", df[\"Target\"].value_counts(dropna=False))\n",
    "\n",
    "# Ensure target is binary {0,1}\n",
    "unique_targets = set(pd.Series(df[\"Target\"]).dropna().unique().tolist())\n",
    "if not unique_targets.issubset({0, 1}):\n",
    "    raise ValueError(f\"Target must be binary (0/1). Found: {unique_targets}\")\n",
    "\n",
    "# Show dtypes summary\n",
    "display(df.dtypes.value_counts())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbaba11",
   "metadata": {},
   "source": [
    "## Step 5.2: Exploratory Data Analysis (EDA)\n",
    "\n",
    "This section covers:\n",
    "- Target distribution (class imbalance)\n",
    "- Summary stats for numeric columns (outliers/odd ranges)\n",
    "- Missing value % per column + missingness visualization\n",
    "- Correlation checks (numeric vs target)\n",
    "- Category distributions for ordinal (0/1/2) + nominal features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ccd59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 5.2: EDA\n",
    "# =========================\n",
    "\n",
    "# 1) Target distribution\n",
    "plt.figure()\n",
    "df[\"Target\"].value_counts().sort_index().plot(kind=\"bar\")\n",
    "plt.title(\"Target Distribution (0=Healthy, 1=Unhealthy)\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# 2) Numeric summary stats\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "display(df[numeric_cols].describe().T)\n",
    "\n",
    "# 3) Missing values % per column\n",
    "missing_pct = (df.isna().mean() * 100).sort_values(ascending=False)\n",
    "display(missing_pct.to_frame(\"missing_%\"))\n",
    "\n",
    "# Missingness \"heatmap\" (matplotlib imshow)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(df.isna().T, aspect=\"auto\", interpolation=\"nearest\")\n",
    "plt.title(\"Missing Values Map (rows vs columns)\")\n",
    "plt.yticks(range(len(df.columns)), df.columns, fontsize=8)\n",
    "plt.xlabel(\"Row index\")\n",
    "plt.show()\n",
    "\n",
    "# 4) Correlation with Target (numeric only)\n",
    "if \"Target\" in numeric_cols:\n",
    "    corr = df[numeric_cols].corr(numeric_only=True)[\"Target\"].sort_values(ascending=False)\n",
    "    display(corr.to_frame(\"corr_with_target\"))\n",
    "\n",
    "    # Correlation matrix plot for numeric columns\n",
    "    corr_matrix = df[numeric_cols].corr(numeric_only=True)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(corr_matrix, aspect=\"auto\")\n",
    "    plt.title(\"Correlation Matrix (Numeric Features)\")\n",
    "    plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90, fontsize=8)\n",
    "    plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns, fontsize=8)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 5) Category distributions (best-effort detection)\n",
    "# Expected ordinal features (0/1/2) — adjust if your dataset uses different names\n",
    "expected_ordinal = [\"Smoking\", \"Alcohol\", \"Diet\", \"MentalHealth\", \"PhysicalActivity\", \"MedicalHistory\", \"Allergies\"]\n",
    "present_ordinal = [c for c in expected_ordinal if c in df.columns]\n",
    "\n",
    "for c in present_ordinal:\n",
    "    plt.figure()\n",
    "    df[c].value_counts(dropna=False).sort_index().plot(kind=\"bar\")\n",
    "    plt.title(f\"Distribution: {c}\")\n",
    "    plt.xlabel(c)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# Nominal features (expected)\n",
    "expected_nominal = [\"Diet_Type\", \"Blood_Group\"]\n",
    "present_nominal = [c for c in expected_nominal if c in df.columns]\n",
    "\n",
    "for c in present_nominal:\n",
    "    plt.figure()\n",
    "    df[c].value_counts(dropna=False).plot(kind=\"bar\")\n",
    "    plt.title(f\"Distribution: {c}\")\n",
    "    plt.xlabel(c)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ EDA complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e4d27",
   "metadata": {},
   "source": [
    "## Step 5.3: Data Cleaning\n",
    "\n",
    "Implements:\n",
    "- Convert **negative Age** to `NaN` (then impute)\n",
    "- Basic sanity checks for impossible values (BMI <= 0, Sleep_Hours out of range, etc.)\n",
    "- Missing value handling via imputers in the modeling pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 5.3: Data Cleaning\n",
    "# =========================\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Convert negative Age to NaN (safe default)\n",
    "if \"Age\" in df_clean.columns:\n",
    "    df_clean.loc[df_clean[\"Age\"] < 0, \"Age\"] = np.nan\n",
    "\n",
    "# Remove / correct impossible values (safe defaults)\n",
    "# You can extend these rules based on domain knowledge.\n",
    "if \"BMI\" in df_clean.columns:\n",
    "    df_clean.loc[df_clean[\"BMI\"] <= 0, \"BMI\"] = np.nan\n",
    "\n",
    "if \"Sleep_Hours\" in df_clean.columns:\n",
    "    df_clean.loc[(df_clean[\"Sleep_Hours\"] < 0) | (df_clean[\"Sleep_Hours\"] > 24), \"Sleep_Hours\"] = np.nan\n",
    "\n",
    "if \"Exercise_Hours\" in df_clean.columns:\n",
    "    df_clean.loc[(df_clean[\"Exercise_Hours\"] < 0) | (df_clean[\"Exercise_Hours\"] > 24), \"Exercise_Hours\"] = np.nan\n",
    "\n",
    "if \"Water_Intake\" in df_clean.columns:\n",
    "    df_clean.loc[(df_clean[\"Water_Intake\"] < 0) | (df_clean[\"Water_Intake\"] > 20), \"Water_Intake\"] = np.nan\n",
    "\n",
    "print(\"✅ Cleaning rules applied (Age negatives, impossible ranges set to NaN).\")\n",
    "print(\"Shape:\", df_clean.shape)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928e4d5",
   "metadata": {},
   "source": [
    "## Step 5.4: Feature Engineering + Encoding\n",
    "\n",
    "Optional engineered features:\n",
    "- BMI category\n",
    "- Blood pressure category\n",
    "- Glucose category\n",
    "- Lifestyle risk score\n",
    "\n",
    "Encoding strategy:\n",
    "- Ordinal 0/1/2 features → keep as numeric (order preserved)\n",
    "- Nominal features (Diet_Type, Blood_Group) → One-Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237ffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Step 5.4: Feature Engineering (Optional)\n",
    "# ======================================\n",
    "df_fe = df_clean.copy()\n",
    "\n",
    "def bmi_category(bmi):\n",
    "    if pd.isna(bmi): return np.nan\n",
    "    if bmi < 18.5: return \"Underweight\"\n",
    "    if bmi < 25: return \"Normal\"\n",
    "    if bmi < 30: return \"Overweight\"\n",
    "    return \"Obese\"\n",
    "\n",
    "def bp_category(sys_bp):\n",
    "    # Using systolic BP categories (simplified)\n",
    "    if pd.isna(sys_bp): return np.nan\n",
    "    if sys_bp < 120: return \"Normal\"\n",
    "    if sys_bp < 130: return \"Elevated\"\n",
    "    if sys_bp < 140: return \"High_Stage1\"\n",
    "    return \"High_Stage2\"\n",
    "\n",
    "def glucose_category(glu):\n",
    "    # Simplified fasting glucose buckets\n",
    "    if pd.isna(glu): return np.nan\n",
    "    if glu < 100: return \"Normal\"\n",
    "    if glu < 126: return \"Prediabetic\"\n",
    "    return \"DiabeticRange\"\n",
    "\n",
    "# Create engineered features if base columns exist\n",
    "if \"BMI\" in df_fe.columns:\n",
    "    df_fe[\"BMI_Category\"] = df_fe[\"BMI\"].apply(bmi_category)\n",
    "\n",
    "if \"Blood_Pressure\" in df_fe.columns:\n",
    "    df_fe[\"BP_Category\"] = df_fe[\"Blood_Pressure\"].apply(bp_category)\n",
    "\n",
    "if \"Glucose_Level\" in df_fe.columns:\n",
    "    df_fe[\"Glucose_Category\"] = df_fe[\"Glucose_Level\"].apply(glucose_category)\n",
    "\n",
    "# Lifestyle risk score (simple additive index) if columns exist\n",
    "lifestyle_cols = [c for c in [\"Smoking\", \"Alcohol\", \"Diet\", \"PhysicalActivity\"] if c in df_fe.columns]\n",
    "if lifestyle_cols:\n",
    "    # Higher smoking/alcohol increases risk, higher diet/physical activity might reduce risk.\n",
    "    # We'll invert Diet and PhysicalActivity to reflect risk (0 poor -> higher risk; 2 good -> lower risk).\n",
    "    risk_score = pd.Series(0, index=df_fe.index, dtype=float)\n",
    "    if \"Smoking\" in df_fe.columns: risk_score += df_fe[\"Smoking\"].fillna(0)\n",
    "    if \"Alcohol\" in df_fe.columns: risk_score += df_fe[\"Alcohol\"].fillna(0)\n",
    "    if \"Diet\" in df_fe.columns: risk_score += (2 - df_fe[\"Diet\"].fillna(1))  # default 1\n",
    "    if \"PhysicalActivity\" in df_fe.columns: risk_score += (2 - df_fe[\"PhysicalActivity\"].fillna(1))\n",
    "    df_fe[\"Lifestyle_Risk_Score\"] = risk_score\n",
    "\n",
    "print(\"✅ Feature Engineering complete.\")\n",
    "df_fe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51d14e",
   "metadata": {},
   "source": [
    "## Step 5.5: Train/Test Split\n",
    "\n",
    "- Uses stratification to preserve class distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 5.5: Train/Test Split\n",
    "# =========================\n",
    "\n",
    "X = df_fe.drop(columns=[\"Target\"])\n",
    "y = df_fe[\"Target\"].astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "print(\"Train target distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Test target distribution:\\n\", y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e74237",
   "metadata": {},
   "source": [
    "## Step 5.6: Baseline Decision Tree Model\n",
    "\n",
    "Build a baseline Decision Tree using a preprocessing pipeline:\n",
    "- Numerical → median imputation\n",
    "- Ordinal (0/1/2) → most frequent imputation (kept numeric)\n",
    "- Nominal → most frequent + one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8a707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# Step 5.6: Baseline Decision Tree Model\n",
    "# =====================================\n",
    "\n",
    "# Identify column types\n",
    "expected_ordinal = [\"Smoking\", \"Alcohol\", \"Diet\", \"MentalHealth\", \"PhysicalActivity\", \"MedicalHistory\", \"Allergies\"]\n",
    "ordinal_cols = [c for c in expected_ordinal if c in X.columns]\n",
    "\n",
    "expected_nominal = [\"Diet_Type\", \"Blood_Group\", \"BMI_Category\", \"BP_Category\", \"Glucose_Category\"]\n",
    "nominal_cols = [c for c in expected_nominal if c in X.columns and X[c].dtype == \"object\"]\n",
    "\n",
    "# Numeric columns = number dtype excluding ordinal_cols\n",
    "numeric_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c]) and c not in ordinal_cols]\n",
    "\n",
    "print(\"Numeric:\", numeric_cols)\n",
    "print(\"Ordinal (0/1/2):\", ordinal_cols)\n",
    "print(\"Nominal:\", nominal_cols)\n",
    "\n",
    "# Preprocessors\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "    # No scaling; keep order\n",
    "])\n",
    "\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"ord\", ordinal_transformer, ordinal_cols),\n",
    "        (\"nom\", nominal_transformer, nominal_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "baseline_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "baseline_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocessor),\n",
    "    (\"model\", baseline_model)\n",
    "])\n",
    "\n",
    "baseline_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(\"✅ Baseline model trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9c27a",
   "metadata": {},
   "source": [
    "## Step 5.7: Hyperparameter Tuning (Control Overfitting)\n",
    "\n",
    "Uses `GridSearchCV` with cross-validation.  \n",
    "Optimizes primarily for **Recall (Unhealthy = 1)** (insurance risk sensitivity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eb7ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# Step 5.7: Hyperparameter Tuning\n",
    "# =====================================\n",
    "\n",
    "param_grid = {\n",
    "    \"model__max_depth\": [3, 4, 5, 6, 8, 10, None],\n",
    "    \"model__min_samples_split\": [2, 5, 10, 20],\n",
    "    \"model__min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"model__max_features\": [None, \"sqrt\", \"log2\"],\n",
    "    \"model__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=baseline_pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"recall\",  # recall for positive class (1) by default in sklearn when labels are {0,1}\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_pipe = grid.best_estimator_\n",
    "print(\"✅ Best params:\", grid.best_params_)\n",
    "print(\"Best CV Recall:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6738f6a2",
   "metadata": {},
   "source": [
    "## Step 5.8: Model Evaluation (Technical + Insurance Lens)\n",
    "\n",
    "Technical metrics:\n",
    "- Confusion Matrix\n",
    "- Accuracy\n",
    "- Precision (Unhealthy)\n",
    "- Recall (Unhealthy) **(priority)**\n",
    "- F1-score\n",
    "- ROC-AUC\n",
    "\n",
    "Insurance lens:\n",
    "- Explicitly count **False Negatives** (Unhealthy predicted as Healthy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea6b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Step 5.8: Evaluation\n",
    "# =========================\n",
    "\n",
    "y_pred = best_pipe.predict(X_test)\n",
    "\n",
    "# If model supports probabilities\n",
    "y_proba = None\n",
    "if hasattr(best_pipe.named_steps[\"model\"], \"predict_proba\"):\n",
    "    y_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}  (Unhealthy=1)\")\n",
    "print(f\"Recall   : {rec:.4f}  (Unhealthy=1)  ✅ priority\")\n",
    "print(f\"F1-score : {f1:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Healthy(0)\", \"Unhealthy(1)\"])\n",
    "disp.plot(values_format=\"d\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(\"Insurance Lens:\")\n",
    "print(\"False Negatives (FN) = Unhealthy predicted as Healthy =\", fn, \"❗ highest financial risk\")\n",
    "\n",
    "# ROC-AUC\n",
    "if y_proba is not None:\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    print(f\"ROC-AUC  : {auc:.4f}\")\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ROC-AUC skipped (predict_proba not available).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f6514f",
   "metadata": {},
   "source": [
    "## Step 5.9: Interpretability Outputs\n",
    "\n",
    "- Feature importance ranking\n",
    "- Visualize tree (limited depth for business readability)\n",
    "- Extract a few simple rules (best-effort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52440867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# Step 5.9: Interpretability Outputs\n",
    "# =====================================\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "pre = best_pipe.named_steps[\"preprocess\"]\n",
    "model = best_pipe.named_steps[\"model\"]\n",
    "\n",
    "feature_names = []\n",
    "\n",
    "# Numeric\n",
    "feature_names += numeric_cols\n",
    "\n",
    "# Ordinal\n",
    "feature_names += ordinal_cols\n",
    "\n",
    "# Nominal (one-hot)\n",
    "if nominal_cols:\n",
    "    ohe = pre.named_transformers_[\"nom\"].named_steps[\"onehot\"]\n",
    "    ohe_names = ohe.get_feature_names_out(nominal_cols).tolist()\n",
    "    feature_names += ohe_names\n",
    "\n",
    "# Feature importance\n",
    "importances = model.feature_importances_\n",
    "fi = pd.DataFrame({\"feature\": feature_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
    "display(fi.head(20))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(min(20, len(fi))), fi[\"importance\"].values[:20])\n",
    "plt.xticks(range(min(20, len(fi))), fi[\"feature\"].values[:20], rotation=90, fontsize=8)\n",
    "plt.title(\"Top 20 Feature Importances (Decision Tree)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot tree (limit depth for readability)\n",
    "plt.figure(figsize=(18, 10))\n",
    "plot_tree(\n",
    "    model,\n",
    "    feature_names=feature_names,\n",
    "    class_names=[\"Healthy(0)\", \"Unhealthy(1)\"],\n",
    "    filled=True,\n",
    "    max_depth=3,\n",
    "    fontsize=8\n",
    ")\n",
    "plt.title(\"Decision Tree (Top Levels, max_depth=3 view)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Interpretability outputs generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c0923",
   "metadata": {},
   "source": [
    "## Step 5.10: Risk Bands + Premium Mapping Logic\n",
    "\n",
    "Probability-based risk bands (recommended):\n",
    "- **Low Risk:** p(Unhealthy) < 0.30\n",
    "- **Medium Risk:** 0.30–0.60\n",
    "- **High Risk:** > 0.60\n",
    "\n",
    "Premium action mapping (example):\n",
    "- Low → Standard / Discount eligible\n",
    "- Medium → Standard + small loading / caution\n",
    "- High → Higher premium / medical review / conditional coverage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe1f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Step 5.10: Risk Bands + Premium Mapping Logic\n",
    "# ============================================\n",
    "\n",
    "def risk_band(p_unhealthy: float) -> str:\n",
    "    if pd.isna(p_unhealthy):\n",
    "        return \"Unknown\"\n",
    "    if p_unhealthy < 0.30:\n",
    "        return \"Low\"\n",
    "    if p_unhealthy <= 0.60:\n",
    "        return \"Medium\"\n",
    "    return \"High\"\n",
    "\n",
    "def premium_action(band: str) -> str:\n",
    "    mapping = {\n",
    "        \"Low\": \"Standard / Discount eligible\",\n",
    "        \"Medium\": \"Standard + small loading / caution\",\n",
    "        \"High\": \"Higher premium / medical review / conditional coverage\",\n",
    "        \"Unknown\": \"Manual review required\"\n",
    "    }\n",
    "    return mapping.get(band, \"Manual review required\")\n",
    "\n",
    "# Create scored test set output table\n",
    "output = X_test.copy()\n",
    "output[\"Actual_Target\"] = y_test.values\n",
    "output[\"Predicted_Target\"] = y_pred\n",
    "\n",
    "if y_proba is not None:\n",
    "    output[\"P_Unhealthy\"] = y_proba\n",
    "    output[\"Risk_Band\"] = output[\"P_Unhealthy\"].apply(risk_band)\n",
    "else:\n",
    "    # Fallback: simple bands based on predicted class\n",
    "    output[\"P_Unhealthy\"] = np.nan\n",
    "    output[\"Risk_Band\"] = np.where(output[\"Predicted_Target\"] == 1, \"High\", \"Low\")\n",
    "\n",
    "output[\"Premium_Action\"] = output[\"Risk_Band\"].apply(premium_action)\n",
    "\n",
    "display(output.head(10))\n",
    "print(\"✅ Risk banding and premium mapping complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f092938",
   "metadata": {},
   "source": [
    "## Step 5.11: Business Validation Checklist (Guidance)\n",
    "\n",
    "Use this checklist with underwriting stakeholders:\n",
    "- Do the top features and splits make medical sense?\n",
    "- Are there unfair/non-actionable patterns?\n",
    "- Is the tree too deep/complex for explanation?\n",
    "- Are False Negatives within acceptable limits?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f5eaf",
   "metadata": {},
   "source": [
    "## Step 5.12: Final Packaging + Export Outputs (Downloadable)\n",
    "\n",
    "This step exports:\n",
    "1. `Anova_DecisionTree_Predictions.csv` (test predictions + risk band + premium action)\n",
    "2. `Anova_DecisionTree_Outputs.xlsx` with **two tabs**:\n",
    "   - `1_Decision Tree Model For Anova Insurance` (predictions + key metrics)\n",
    "   - `2_Decision Tree Success Criteria` (technical + business success criteria)\n",
    "\n",
    "If running in Colab, it triggers downloads automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2461ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Export outputs for download (CSV + Excel with 2 tabs)\n",
    "# =====================================================\n",
    "\n",
    "# 1) Save predictions CSV\n",
    "pred_csv_path = \"Anova_DecisionTree_Predictions.csv\"\n",
    "output.to_csv(pred_csv_path, index=False)\n",
    "print(\"✅ Saved:\", pred_csv_path)\n",
    "\n",
    "# 2) Build Success Criteria sheet (Step 6)\n",
    "success_criteria = pd.DataFrame({\n",
    "    \"Category\": [\n",
    "        \"Technical\", \"Technical\", \"Technical\", \"Technical\", \"Technical\",\n",
    "        \"Business\", \"Business\", \"Business\"\n",
    "    ],\n",
    "    \"Success_Criteria\": [\n",
    "        \"Accuracy (baseline indicator)\",\n",
    "        \"Recall for Unhealthy (primary)\",\n",
    "        \"Precision + F1-score (balanced performance)\",\n",
    "        \"ROC-AUC (separation capability)\",\n",
    "        \"Low False Negatives (unhealthy predicted healthy)\",\n",
    "        \"Better premium differentiation\",\n",
    "        \"Reduced underwriting risk\",\n",
    "        \"Scalable + explainable health scoring\"\n",
    "    ],\n",
    "    \"How_to_Measure\": [\n",
    "        \"accuracy_score on test set\",\n",
    "        \"recall_score(pos_label=1) on test set\",\n",
    "        \"precision_score & f1_score(pos_label=1) on test set\",\n",
    "        \"roc_auc_score using predicted probabilities\",\n",
    "        \"Confusion matrix FN count should be minimized\",\n",
    "        \"Premium tiers align with risk bands and outcomes\",\n",
    "        \"Lower claim losses from underpriced high-risk policies (business KPI)\",\n",
    "        \"Stakeholder adoption + auditability of tree rules\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# 3) Create a metrics summary to include in Sheet 1\n",
    "metrics_summary = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision (Unhealthy=1)\", \"Recall (Unhealthy=1)\", \"F1 (Unhealthy=1)\", \"False Negatives (FN)\", \"True Positives (TP)\", \"False Positives (FP)\", \"True Negatives (TN)\"],\n",
    "    \"Value\": [acc, prec, rec, f1, int(fn), int(tp), int(fp), int(tn)]\n",
    "})\n",
    "\n",
    "if y_proba is not None:\n",
    "    metrics_summary = pd.concat([metrics_summary, pd.DataFrame({\"Metric\":[\"ROC-AUC\"], \"Value\":[auc]})], ignore_index=True)\n",
    "\n",
    "# 4) Export Excel with two tabs\n",
    "xlsx_path = \"Anova_DecisionTree_Outputs.xlsx\"\n",
    "sheet1_name = \"1_Decision Tree Model For Anova Insurance\"\n",
    "sheet2_name = \"2_Decision Tree Success Criteria\"\n",
    "\n",
    "with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as writer:\n",
    "    # Sheet 1: metrics + predictions\n",
    "    metrics_summary.to_excel(writer, sheet_name=sheet1_name, index=False, startrow=0)\n",
    "    # Leave a gap, then write predictions under metrics\n",
    "    startrow = len(metrics_summary) + 3\n",
    "    output.to_excel(writer, sheet_name=sheet1_name, index=False, startrow=startrow)\n",
    "\n",
    "    # Sheet 2: success criteria table\n",
    "    success_criteria.to_excel(writer, sheet_name=sheet2_name, index=False)\n",
    "\n",
    "print(\"✅ Saved:\", xlsx_path)\n",
    "\n",
    "# 5) Download files (Colab)\n",
    "if IN_COLAB:\n",
    "    files.download(pred_csv_path)\n",
    "    files.download(xlsx_path)\n",
    "else:\n",
    "    print(\"Run in Colab to auto-download, or find files in your working directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c364503a",
   "metadata": {},
   "source": [
    "## Step 1 (last item): Deployment Readiness + Monitoring (Guidance)\n",
    "\n",
    "When you productionize:\n",
    "- Save the trained pipeline (preprocessor + model)\n",
    "- Version dataset + model parameters\n",
    "- Monitor:\n",
    "  - data drift (feature distributions)\n",
    "  - performance drift (recall for Unhealthy)\n",
    "  - FN rate over time (highest insurer risk)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
